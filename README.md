# MLOPS Assignment 2 : Group 111
SHAIK MAHAMMED ASIF : 2024aa05500

BHAVNA YADAV        : 2023ac05950

K JAGADEESH KUMAR   : 2024aa05124

M SWATI RANI        : 2024aa05305

V N SANJAY          : 2024aa05123

---

# ğŸ¶ğŸ± Cats vs Dogs â€“ End-to-End MLOps Pipeline

This project implements a complete **end-to-end MLOps pipeline** for a binary image classification task (Cats vs Dogs) designed for a pet adoption platform.

It demonstrates model development, experiment tracking, packaging, containerization, CI/CD automation, deployment validation, and post-deployment monitoring using open-source tools.

---

# ğŸ“Œ Project Overview

**Use Case:**
Binary classification of cat and dog images.

**Objective:**
Design and implement a reproducible and automated MLOps pipeline using industry-standard open-source tools.

---

# ğŸ— End-to-End Architecture

Data â†’ Model Training â†’ MLflow Tracking â†’ Model Artifact
â†“
FastAPI Inference Service
â†“
Docker Containerization
â†“
GitHub Actions CI/CD
â†“
Deployment + Smoke Testing
â†“
Post-Deployment Monitoring

---

# ğŸ“‚ Project Structure

```
mlops-cats-dogs-project/
â”‚
â”œâ”€â”€ app/                      # FastAPI inference service
â”œâ”€â”€ src/                      # Model + training code
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ test_images/              # Images for post-deployment evaluation
â”‚
â”œâ”€â”€ train.py                  # Model training + MLflow logging
â”œâ”€â”€ evaluate_post_deploy.py   # Production monitoring script
â”œâ”€â”€ smoke_check.py            # Smoke test script
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ .github/workflows/ci.yml  # CI/CD pipeline
â”œâ”€â”€ data.dvc                  # DVC dataset tracking
â””â”€â”€ README.md
```

---

# ğŸ§  M1 â€“ Model Development & Experiment Tracking

## âœ… Model

* Baseline CNN implemented using PyTorch
* Input images resized to 224x224 RGB
* Model saved as `model.pt`

## âœ… Experiment Tracking (MLflow)

Logged:

* Hyperparameters (epochs, learning rate, batch size)
* Training loss
* Training accuracy
* Loss curve artifact
* Confusion matrix artifact
* Trained model artifact

### Run Training

```
python src/train.py
mlflow ui
```

Open in browser:

```
http://127.0.0.1:5000
```

---

# ğŸ“¦ M2 â€“ Model Packaging & Containerization

## âœ… FastAPI Inference Service

Endpoints:

| Endpoint   | Description                                    |
| ---------- | ---------------------------------------------- |
| `/health`  | Health check                                   |
| `/predict` | Returns predicted class + confidence + latency |
| `/metrics` | Returns total request count                    |

### Run API Locally

```
uvicorn app.main:app --reload
```

Open:

```
http://127.0.0.1:8000/docs
```

---

## âœ… Docker

Build Image:

```
docker build -t cats-dogs .
```

Run Container:

```
docker run -p 8000:8000 cats-dogs
```

---

# ğŸ”„ M3 â€“ Continuous Integration (CI)

Implemented using **GitHub Actions**.

Pipeline automatically:

* Installs dependencies
* Runs unit tests (pytest)
* Builds Docker image
* Logs into DockerHub
* Pushes image to registry
* Starts container
* Executes smoke test

Triggered on every push to `main`.

---

# ğŸš€ M4 â€“ Continuous Deployment (CD)

* Docker image automatically pushed to DockerHub
* Container deployed during CI
* Smoke test validates API availability
* Deployment script (`deploy.sh`) included

---

# ğŸ“Š M5 â€“ Monitoring & Post-Deployment Tracking

## âœ… Request Logging

Each API call logs:

* Request number
* Latency

Example:

```
Request #1 | Latency: 0.0404s
```

---

## âœ… Monitoring Endpoint

```
GET /metrics
```

Returns:

```json
{
  "total_requests": 5
}
```

---

## âœ… Post-Deployment Performance Tracking

Script:

```
python evaluate_post_deploy.py
```

This:

* Sends test images to deployed API
* Compares predicted vs true labels
* Calculates production accuracy
* Logs metric to MLflow

Example output:

```
Post-Deployment Accuracy: 0.25
```

---

# ğŸ“¦ Dataset Versioning (DVC)

Dataset tracked using DVC:

```
dvc init
dvc add data/
git add data.dvc
```

Ensures reproducible dataset management separate from source code.

---

# ğŸ›  Technologies Used

* Python
* PyTorch
* FastAPI
* MLflow
* DVC
* Docker
* GitHub Actions
* Pytest

---

# â–¶ï¸ How To Run End-to-End

### 1ï¸âƒ£ Train Model

```
python src/train.py
```

### 2ï¸âƒ£ Start API

```
uvicorn app.main:app --reload
```

### 3ï¸âƒ£ Test Prediction

```
curl -X POST -F "file=@dog.jpg" http://127.0.0.1:8000/predict
```

### 4ï¸âƒ£ Monitor Requests

Open:

```
http://127.0.0.1:8000/metrics
```

### 5ï¸âƒ£ Run Post-Deployment Evaluation

```
python evaluate_post_deploy.py
```

---

# ğŸ“ Assignment Coverage

| Module                    | Status |
| ------------------------- | ------ |
| M1 â€“ Model & Tracking     | âœ…      |
| M2 â€“ Packaging & Docker   | âœ…      |
| M3 â€“ CI Pipeline          | âœ…      |
| M4 â€“ CD Deployment        | âœ…      |
| M5 â€“ Monitoring & Logging | âœ…      |

---

# ğŸ† Conclusion

This project demonstrates a complete, automated, and reproducible MLOps pipeline integrating:

* Model training
* Experiment tracking
* API development
* Docker containerization
* CI/CD automation
* Deployment validation
* Production monitoring

It reflects real-world MLOps practices used in industry systems.

---

## ğŸ‘©â€ğŸ’» Author

Bhavna Ya